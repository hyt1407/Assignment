{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下定义生成器与判别器的网络结构\n",
    "\n",
    "- 生成器和判别器均使用孪生网络  \n",
    "- 生成器使用RNN方式，第一个step生成local文件，第二个step生成全局图像。  \n",
    "- 生成器step1和step2使用同一个骨干网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每个local的输出的分支网络（如果需要的话）（如果输入也要分支也可以用这个）\n",
    "class outBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels):\n",
    "        super(outBlock,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels,32,kernel_size=(3,3),stride=1,padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential(\n",
    "        nn.Conv2d(32,64,kernel_size=1,stride=1,padding=0),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block = nn.Sequential(\n",
    "        nn.Conv2d(32,64,kernel_size = 1,stride = 1,padding = 0),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Conv2d(64,64,kernel_size = 3,stride = 1,padding = 1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.sqush = nn.Conv2d(128,out_channels,kernel_size=3,stride=1,padding=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x1 = self.shortcut(x)\n",
    "        x2 = self.block(x)\n",
    "        x = torch.cat((x1,x2),1)\n",
    "        return self.sqush(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注册钩子函数\n",
    "class saveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unetUpSampleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    用于创建unet右侧的上采样层，采用转置卷积进行上采样（尺寸×2）\n",
    "    self.tranConv将上一层进行上采样，尺寸×2\n",
    "    self.conv，将左侧特征图再做一次卷积减少通道数，所以尺寸不变\n",
    "    此时两者尺寸正好一致-----建立在图片尺寸为128×128的基础上，否则上采样不能简单的×2\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels,feature_channels,out_channels,dp=False,ps=0.25):#注意，out_channels 是最终输出通道的一半。\n",
    "        super(unetUpSampleBlock,self).__init__()\n",
    "        self.tranConv = nn.ConvTranspose2d(in_channels,out_channels,kernel_size=2,stride=2,bias=False)#输出尺寸正好为输入尺寸的两倍\n",
    "        self.conv = nn.Conv2d(feature_channels,out_channels,1,bias=False) #这一层将传来的特征图再做一次卷积，将特征图通道数减半\n",
    "        self.bn = nn.BatchNorm2d(out_channels*2) #将特征图与上采样再通道出相加后再一起归一化\n",
    "        self.dp = dp\n",
    "        if dp:\n",
    "            self.dropout = nn.Dropout(ps,inplace=True)\n",
    "            \n",
    "    def forward(self,x,features):\n",
    "        x1 = self.tranConv(x)\n",
    "        x2 = self.conv(features)\n",
    "        x = torch.cat([x1,x2],dim=1)\n",
    "        x = self.bn(F.relu(x))\n",
    "        return self.dropout(x) if self.dp else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    #基于resnet50的UNet网络\n",
    "    #NIR是可见光模式，3通道\n",
    "    #主干网络为Unet，输入输出尺寸均为64×64\n",
    "    def __init__(self,model,in_channels,out_channels):\n",
    "        super(Generator,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels,64,kernel_size=3,stride=1,padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(inplace = True)\n",
    "        )\n",
    "        self.downsample = nn.Sequential(*list(model.children())[4:-2])\n",
    "        #print(len(list(model.children())[4:-2]))\n",
    "        self.features = [saveFeatures(list(self.downsample.children())[i]) for i in range(3)]\n",
    "        self.up1 = unetUpSampleBlock(2048,1024,512) #feature:self.features[2]\n",
    "        self.up2 = unetUpSampleBlock(1024,512,256)\n",
    "        self.up3 = unetUpSampleBlock(512,256,128)\n",
    "        self.up4 = unetUpSampleBlock(256,64,32) #feature:self.layer1的输出\n",
    "        self.outlayer = nn.Conv2d(64,out_channels,3,1,1)\n",
    "        \n",
    "    def forward(self,batch):\n",
    "        out_batch = []\n",
    "        for i in batch:\n",
    "            x1 = self.layer1(i)\n",
    "            x = self.downsample(x1)\n",
    "            x = self.up1(x,self.features[2].features)\n",
    "            x = self.up2(x,self.features[1].features)\n",
    "            x = self.up3(x,self.features[0].features)\n",
    "            x = self.up4(x,x1)\n",
    "            out_batch.append(self.outlayer(x))\n",
    "        return out_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models.resnet50()\n",
    "tem_paras = copy.deepcopy(m.layer1[0].downsample[0].state_dict())\n",
    "m.layer1[0].downsample[0] = nn.Conv2d(64, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "m.layer1[0].downsample[0].load_state_dict(tem_paras)\n",
    "tem_paras = copy.deepcopy(m.layer1[0].conv2.state_dict())\n",
    "m.layer1[0].conv2 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "m.layer1[0].conv2.load_state_dict(tem_paras)\n",
    "del tem_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m = Generator(m,3,1)  #------生成一个生成器\n",
    "f = torch.ones(2,3,32,16)\n",
    "t = torch.ones(2,3,32,16)\n",
    "f = m([f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 32, 16])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下定义数据读取\n",
    "\n",
    "- 分别读取一张图片即上面的头、胸、手、腿  \n",
    "- 全局图片为.resize((64,128))  \n",
    "- 头为.resize((32,16))\n",
    "- 胸部为.resize((64,64))  \n",
    "- 手臂为.resize((64,64))  \n",
    "- 腿部为.resize((64,128))  \n",
    "- 坐标文件存储在images_NIR.yml和images_VIS.yml两个文件上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(x1,y1,x2,y2):\n",
    "    return x1,y1,x2,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasets(Dataset):\n",
    "    def __init__(self,img_NIR_all,img_VIS_all,img_train_NIR_list,img_train_VIS_list,img_NIR_dir,img_VIS_dir):\n",
    "        self.img_NIR_all = img_NIR_all\n",
    "        self.img_VIS_all = img_VIS_all\n",
    "        self.img_train_NIR_list = img_train_NIR_list\n",
    "        self.img_train_VIS_list = img_train_VIS_list\n",
    "        self.img_NIR_dir = img_NIR_dir\n",
    "        self.img_VIS_dir = img_VIS_dir\n",
    "        self.NIR_key = list(img_NIR_all.keys())\n",
    "        self.VIS_key = list(img_VIS_all.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_train_NIR_list)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_NIR_info = self.img_NIR_all[self.NIR_key[idx]]\n",
    "        img_VIS_info = self.img_VIS_all[self.VIS_key[idx]]\n",
    "        \n",
    "        batch = {}\n",
    "        \n",
    "        name = self.NIR_key[idx].split('.')\n",
    "        name = name[0][:-2]+'.'+name[1]\n",
    "        batch['img_NIR'] = Image.open(os.path.join(self.img_NIR_dir,name)).convert('L').resize((64,128))\n",
    "        #如果想要打乱NIR图像与VIS图像之间的关系的话只需重新随机选择一个idx即可\n",
    "        name = self.VIS_key[idx].split('.')\n",
    "        name = name[0][:-2]+'.'+name[1]\n",
    "        batch['img_VIS'] = Image.open(os.path.join(self.img_VIS_dir,name)).convert('RGB').resize((64,128))\n",
    "        \n",
    "        batch['id_NIR'] = int(self.NIR_key[idx].split('_')[0])\n",
    "        batch['id_VIS'] = int(self.VIS_key[idx].split('_')[0])\n",
    "        \n",
    "        batch['head_NIR'] = batch['img_NIR'].crop(process(**img_NIR_info['head'])).resize((32,16))\n",
    "        batch['head_VIS'] = batch['img_VIS'].crop(process(**img_NIR_info['head'])).resize((32,16))\n",
    "        \n",
    "        batch['chest_NIR'] = batch['img_NIR'].crop(process(**img_NIR_info['chest'])).resize((64,64))\n",
    "        batch['chest_VIS'] = batch['img_VIS'].crop(process(**img_NIR_info['chest'])).resize((64,64))\n",
    "        \n",
    "        batch['thigh_NIR'] = batch['img_NIR'].crop(process(**img_NIR_info['thigh'])).resize((64,64))\n",
    "        batch['thigh_VIS'] = batch['img_VIS'].crop(process(**img_NIR_info['thigh'])).resize((64,64))\n",
    "        \n",
    "        batch['leg_NIR'] = batch['img_NIR'].crop(process(**img_NIR_info['leg'])).resize((64,128))\n",
    "        batch['leg_VIS'] = batch['img_VIS'].crop(process(**img_NIR_info['leg'])).resize((64,128))\n",
    "        \n",
    "        totensor = transforms.ToTensor()\n",
    "        for i in batch.keys():\n",
    "            if i == 'id_NIR' or i == 'id_VIS':\n",
    "                continue\n",
    "            batch[i] = totensor(batch[i])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDatasets(yaml_NIR,yaml_VIS,img_NIR_dir,img_VIS_dir,p_test=0.1):\n",
    "    with open(yaml_NIR,'r') as rf:\n",
    "        img_NIR_all = yaml.safe_load(rf.read())\n",
    "    with open(yaml_VIS,'r') as rf:\n",
    "        img_VIS_all = yaml.safe_load(rf.read())\n",
    "        \n",
    "    #假设img_NIR_all和img_VIS_all长度一致\n",
    "    length = min(len(img_NIR_all),len(img_VIS_all))\n",
    "    \n",
    "    img_test_NIR_list = list(img_NIR_all.keys())[:int(length*p_test)]\n",
    "    img_test_VIS_list = list(img_VIS_all.keys())[:int(length*p_test)]\n",
    "    img_train_NIR_list = list(img_NIR_all.keys())[int(length*p_test):length]\n",
    "    img_train_VIS_list = list(img_VIS_all.keys())[int(length*p_test):length]\n",
    "    #return img_NIR_all,img_VIS_all,img_train_NIR_list,img_train_VIS_list,img_NIR_dir,img_VIS_dir\n",
    "    return CustomDatasets(img_NIR_all,img_VIS_all,img_train_NIR_list,img_train_VIS_list,img_NIR_dir,img_VIS_dir),CustomDatasets(img_NIR_all,img_VIS_all,img_test_NIR_list,img_test_VIS_list,img_NIR_dir,img_VIS_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_patch(batch):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下定义训练及测试\n",
    "\n",
    "- 训练过程：先生成5各VIS图，再把它们合并再生成更清楚的全局VIS图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_genernator(genernator_A,genernator_B,merge_A,mearge_B,discriminator_A,data_batch):\n",
    "    fake_VIS_1 = genernator_A([data_batch['img_NIR'],data_batch['head_NIR'],data_batch['chest_NIR'],data_batch['thigh_NIR'],data_batch['leg_NIR']])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
