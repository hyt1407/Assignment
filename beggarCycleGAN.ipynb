{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单图片生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class saveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unetUpSampleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    用于创建unet右侧的上采样层，采用转置卷积进行上采样（尺寸×2）\n",
    "    self.tranConv将上一层进行上采样，尺寸×2\n",
    "    self.conv，将左侧特征图再做一次卷积减少通道数，所以尺寸不变\n",
    "    此时两者尺寸正好一致-----建立在图片尺寸为128×128的基础上，否则上采样不能简单的×2\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels,feature_channels,out_channels,dp=False,ps=0.25):#注意，out_channels 是最终输出通道的一半。\n",
    "        super(unetUpSampleBlock,self).__init__()\n",
    "        self.tranConv = nn.ConvTranspose2d(in_channels,out_channels,kernel_size=2,stride=2,bias=False)#输出尺寸正好为输入尺寸的两倍\n",
    "        self.conv = nn.Conv2d(feature_channels,out_channels,1,bias=False) #这一层将传来的特征图再做一次卷积，将特征图通道数减半\n",
    "        self.bn = nn.BatchNorm2d(out_channels*2) #将特征图与上采样再通道出相加后再一起归一化\n",
    "        self.dp = dp\n",
    "        if dp:\n",
    "            self.dropout = nn.Dropout(ps,inplace=True)\n",
    "            \n",
    "    def forward(self,x,features):\n",
    "        x1 = self.tranConv(x)\n",
    "        x2 = self.conv(features)\n",
    "        x = torch.cat([x1,x2],dim=1)\n",
    "        x = self.bn(F.relu(x))\n",
    "        return self.dropout(x) if self.dp else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    #基于resnet50的UNet网络\n",
    "    #NIR是可见光模式，3通道\n",
    "    #主干网络为Unet，输入输出尺寸均为64×64\n",
    "    def __init__(self,model,in_channels,out_channels):\n",
    "        super(Generator,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels,64,kernel_size=3,stride=1,padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(inplace = True)\n",
    "        )\n",
    "        self.downsample = nn.Sequential(*list(model.children())[4:-2])\n",
    "        #print(len(list(model.children())[4:-2]))\n",
    "        self.features = [saveFeatures(list(self.downsample.children())[i]) for i in range(3)]\n",
    "        self.up1 = unetUpSampleBlock(2048,1024,512) #feature:self.features[2]\n",
    "        self.up2 = unetUpSampleBlock(1024,512,256)\n",
    "        self.up3 = unetUpSampleBlock(512,256,128)\n",
    "        self.up4 = unetUpSampleBlock(256,64,32) #feature:self.layer1的输出\n",
    "        self.outlayer = nn.Conv2d(64,out_channels,3,1,1)\n",
    "        \n",
    "    def forward(self,i):\n",
    "        x1 = self.layer1(i)\n",
    "        x = self.downsample(x1)\n",
    "        x = self.up1(x,self.features[2].features)\n",
    "        x = self.up2(x,self.features[1].features)\n",
    "        x = self.up3(x,self.features[0].features)\n",
    "        x = self.up4(x,x1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = models.resnet50(pretrained=True)\n",
    "tem_paras = copy.deepcopy(m.layer1[0].downsample[0].state_dict())\n",
    "m.layer1[0].downsample[0] = nn.Conv2d(64, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "m.layer1[0].downsample[0].load_state_dict(tem_paras)\n",
    "tem_paras = copy.deepcopy(m.layer1[0].conv2.state_dict())\n",
    "m.layer1[0].conv2 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "m.layer1[0].conv2.load_state_dict(tem_paras)\n",
    "del tem_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "genernator_VIS2NIR = Generator(m,3,1)\n",
    "genernator_NIR2VIS = Generator(m,1,3)\n",
    "discriminator_A_NIR = models.resnet34(pretrained=True)\n",
    "discriminator_B_VIS = models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_A_NIR.fc = nn.Linear(512,1,bias = True)\n",
    "discriminator_B_VIS.fc = nn.Linear(512,1,bias = True)\n",
    "#resnet降的倍数太多了，减少一个pool\n",
    "discriminator_B_VIS.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
    "discriminator_A_NIR.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genernator_VIS2NIR = nn.DataParallel(genernator_VIS2NIR).cuda()\n",
    "genernator_NIR2VIS = nn.DataParallel(genernator_NIR2VIS).cuda()\n",
    "discriminator_A_NIR = nn.DataParallel(discriminator_A_NIR).cuda()\n",
    "discriminator_B_VIS = nn.DataParallel(discriminator_B_VIS).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import yaml\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasets(Dataset):\n",
    "    def __init__(self,img_NIR_dir,img_VIS_dir,NIR_list,VIS_list):\n",
    "        self.img_NIR_dir = img_NIR_dir\n",
    "        self.img_VIS_dir = img_VIS_dir\n",
    "        self.NIR_list = NIR_list\n",
    "        self.VIS_list = VIS_list\n",
    "    def __len__(self):\n",
    "        return len(self.NIR_list)\n",
    "    def __getitem__(self,idx):\n",
    "        NIR = Image.open(os.path.join(self.img_NIR_dir,self.NIR_list[idx])).convert('L').resize((64,128))\n",
    "        VIS = Image.open(os.path.join(self.img_VIS_dir,self.VIS_list[idx])).convert('RGB').resize((64,128))\n",
    "        \n",
    "        totensor = transforms.ToTensor()\n",
    "        return totensor(VIS),totensor(NIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(img_NIR_dir,img_VIS_dir,p = 0.1):\n",
    "    NIR_list = os.listdir(img_NIR_dir)\n",
    "    VIS_list = os.listdir(img_VIS_dir)\n",
    "    l = int(15513*(1-p))\n",
    "    return CustomDatasets(img_NIR_dir,img_VIS_dir,NIR_list[:l],VIS_list[:l]),CustomDatasets(img_NIR_dir,img_VIS_dir,NIR_list[l:],VIS_list[l:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet,testSet = createDataset('./data/trainB/','./data/trainA/')\n",
    "train_loader = data.DataLoader(trainSet,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader = data.DataLoader(testSet,1,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下定义训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果只是L1范数，则loss会特别大，可以改用mean(abs(map))\n",
    "def similarity_loss(real,fake):\n",
    "    loss = 0\n",
    "    for i,j in zip(real,fake):\n",
    "        loss += torch.mean(torch.abs(i-j))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_loss(discrinminator,fake):\n",
    "    loss = 0\n",
    "    for i in fake:\n",
    "        loss += torch.pow(discrinminator(i.expand(-1,3,-1,-1))-1,2) \n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genernator_train(genernator,discriminator,optim,data):\n",
    "    genernator[0].train()\n",
    "    genernator[1].train()\n",
    "    discriminator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    VIS2NIR_A_fake = genernator[0](data[0])\n",
    "    NIR2VIS_B_fake = genernator[1](VIS2NIR_A_fake)\n",
    "    simil_loss_A = similarity_loss(data[0],NIR2VIS_B_fake)\n",
    "    dis_loss_A = score_loss(discriminator[0],VIS2NIR_A_fake)\n",
    "    loss = simil_loss_A+dis_loss_A\n",
    "    optim[0].zero_grad()\n",
    "    loss.backward()\n",
    "    optim[0].step()\n",
    "    del VIS2NIR_A_fake,NIR2VIS_B_fake\n",
    "    \n",
    "    NIR2VIS_B_fake = genernator[1](data[1])\n",
    "    VIS2NIR_A_fake = genernator[0](NIR2VIS_B_fake)\n",
    "    simil_loss_B = similarity_loss(data[1],NIR2VIS_B_fake)\n",
    "    dis_loss_B = score_loss(discriminator[1],NIR2VIS_B_fake)\n",
    "    loss = simil_loss_B+dis_loss_B\n",
    "    optim[0].zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return simil_loss_A,dis_loss_A,simil_loss_B,dis_loss_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(discriminator,fake,real):\n",
    "    loss = 0\n",
    "    for i,j in zip(fake,real):\n",
    "        #print(j.expand(-1,3,-1,-1).shape)\n",
    "        loss += (torch.pow(discriminator(j.expand(-1,3,-1,-1))-1,2)+torch.pow(discriminator(i.expand(-1,3,-1,-1)),2))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train(genernator,discriminator,optim,data):\n",
    "    discriminator[0].train()\n",
    "    discriminator[1].train()\n",
    "    genernator[0].eval()\n",
    "    genernator[1].eval()\n",
    "    \n",
    "    VIS2NIR_fake = genernator[0](data[0]).detach()\n",
    "    \n",
    "    loss_A = discriminator_loss(discriminator[0],VIS2NIR_fake,data[0])\n",
    "    del VIS2NIR_fake\n",
    "    optim[0].zero_grad()\n",
    "    loss_A.backward()\n",
    "    optim[0].step()\n",
    "    NIR2VIS_fake = genernator[1](data[1]).detach()\n",
    "    loss_B = discriminator_loss(discriminator[1],NIR2VIS_fake.data[1])\n",
    "    del NIR2VIS_fake\n",
    "    optim[1].zero_grad()\n",
    "    loss_B.backward()\n",
    "    optim[1].step()\n",
    "    \n",
    "    return loss_A,loss_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(genernator,data,epoch):\n",
    "    genernator[0].eval()\n",
    "    genernator[1].eval()\n",
    "    \n",
    "    transform = transforms.ToPILImage()\n",
    "    with torch.no_grad():\n",
    "        VIS2NIR_fake = genernator[0](data[0])\n",
    "        NIR2VIS_fake = genernator[1](VIS2NIR_fake)\n",
    "    fig=plt.figure(figsize=(16, 4))\n",
    "    columns = 4\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(transform(data[0].cpu()))\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(transform(VIS2NIR_fake.cpu()))\n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "    plt.imshow(transform(NIR2VIS_fake.cpu()))\n",
    "    fig.add_subplot(rows, columns, 4)\n",
    "    plt.imshow(transform(data[1].cpu()))\n",
    "    plt.tight_layout()       \n",
    "    plt.savefig('./process_image/VIS2NIR_A_%d.jpg'%(epoch+1))\n",
    "    plt.show()\n",
    "    with torch.no_grad():\n",
    "        NIR2VIS_fake = genernator[1](data[1])\n",
    "        VIS2NIR_fake = genernator[0](data[0])\n",
    "    fig=plt.figure(figsize=(16, 4))\n",
    "    columns = 4\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(transform(data[1].cpu()))\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(transform(NIR2VIS_fake.cpu()))\n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "    plt.imshow(transform(VIS2NIR_fake.cpu()))\n",
    "    fig.add_subplot(rows, columns, 4)\n",
    "    plt.imshow(transform(data[0].cpu()))\n",
    "    plt.tight_layout()       \n",
    "    plt.savefig('./process_image/NIR2VIS_B_%d.jpg'%(epoch+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimzer_gen_A_VIS2NIR = optim.RMSprop(genernator_VIS2NIR.parameters(),lr=0.0002)\n",
    "optimzer_gen_B_NIR2VIS = optim.RMSprop(genernator_NIR2VIS.parameters(),lr=0.0002)\n",
    "optimzer_dis_A = optim.Adam(discriminator_A_NIR.parameters(),lr = 0.001)\n",
    "optimzer_dis_B = optim.Adam(discriminator_B_VIS.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in EPOCHS:\n",
    "    test([genernator_VIS2NIR,genernator_NIR2VIS],next(iter(test_loader)),epoch)\n",
    "    for data in train_loader:\n",
    "        data[0] = data[0].to(device)\n",
    "        data[1] = data[1].to(device)\n",
    "        simil_loss_A,dis_loss_A,simil_loss_B,dis_loss_B = genernator_train(\n",
    "            [genernator_VIS2NIR,genernator_NIR2VIS],\n",
    "            [discriminator_A_NIR,discriminator_B_VIS],\n",
    "            [optimzer_gen_A_VIS2NIR,optimzer_gen_B_NIR2VIS],\n",
    "            data)\n",
    "        loss_A,loss_B = discriminator_train(\n",
    "            [genernator_VIS2NIR,genernator_NIR2VIS],\n",
    "            [discriminator_A_NIR,discriminator_B_VIS],\n",
    "            [optimzer_gen_A_VIS2NIR,optimzer_gen_B_NIR2VIS],\n",
    "            data)\n",
    "    print('epoch: {}/{},loss_A_consistency: {},loss_A_discriminator: {},loss_B_consistency: {},loss_B_discriminator: {}'.format(epoch+1,EPOCHS,simil_loss_A,dis_loss_A.item(),simil_loss_B,dis_loss_B.item()))\n",
    "    print('discriminator_A_VIS_loss:{},discriminator_B_NIR_loss{}'.format(loss_A.item(),loss_B.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
